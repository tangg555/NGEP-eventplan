"""
@Desc:
@Reference:
@Notes:
"""

import os
from collections import defaultdict, Counter
from typing import Callable, Dict, Iterable, List, Tuple, Union
import numpy as np

from rouge_score import rouge_scorer, scoring
import nltk

from src.utils.string_utils import rm_extra_spaces

ROUGE_KEYS = ["rouge1", "rouge2", "rougeL", "rougeLsum"]

def line_normalize(line: str):
    line = " ".join(line.strip().split())
    return line

def calculate_bleu(ref_lines, gen_lines, metrics: dict = None):
    if metrics is None:
        metrics = {}
    for bleu_i in range(1, 5):
        weights = tuple([1. / bleu_i for _ in range(bleu_i)])
        metrics[f"bleu-{bleu_i}"] = round(nltk.translate.bleu_score.corpus_bleu(
            list_of_references=[[ref] for ref in ref_lines],
            hypotheses=gen_lines,
            weights=weights), 4)
    return metrics

def extract_rouge_mid_statistics(dct):
    new_dict = {}
    for k1, v1 in dct.items():
        mid = v1.mid
        new_dict[k1] = {stat: round(getattr(mid, stat), 4) for stat in ["precision", "recall", "fmeasure"]}
    return new_dict

def calculate_rouge(
    pred_lines: List[str],
    tgt_lines: List[str],
    use_stemmer=True,
    rouge_keys=ROUGE_KEYS,
    return_precision_and_recall=False,
    bootstrap_aggregation=True,
    newline_sep=True,
) -> Dict:
    """Calculate rouge using rouge_scorer package.

    Args:
        pred_lines: list of summaries generated by model
        tgt_lines: list of groundtruth summaries (e.g. contents of val.target)
        use_stemmer:  Bool indicating whether Porter stemmer should be used to
        strip word suffixes to improve matching.
        rouge_keys:  which metrics to compute, defaults to rouge1, rouge2, rougeL, rougeLsum
        return_precision_and_recall: (False) whether to also return precision and recall.
        bootstrap_aggregation: whether to do the typical bootstrap resampling of scores. Defaults to True, if False
            this function returns a collections.defaultdict[metric: list of values for each observation for each subscore]``
        newline_sep:(default=True) whether to add newline between sentences. This is essential for calculation rougeL
        on multi sentence summaries (CNN/DM dataset).

    Returns:
         Dict[score: value] if aggregate else defaultdict(list) keyed by rouge_keys

    """
    scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)
    aggregator = scoring.BootstrapAggregator()
    for tgt, pred in zip(tgt_lines, pred_lines):
        # rougeLsum expects "\n" separated sentences within a summary
        if newline_sep:
            pred = pred + "\n"
            tgt = tgt + "\n"
        scores = scorer.score(target=tgt, prediction=pred)
        aggregator.add_scores(scores)

    if bootstrap_aggregation:
        result = aggregator.aggregate()
        if return_precision_and_recall:
            return extract_rouge_mid_statistics(result)  # here we return dict
        else:
            return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}

    else:
        return aggregator._scores  # here we return defaultdict(list)


def repetition_distinction_metric(gen_lines:List[List], metrics: dict = None, repetition_times=2):
    if metrics is None:
        metrics = {}

    for gram_n in range(1, 5):
        repetition_count = 0
        distinct_ngram = defaultdict(int)
        all_ngram_num = 0
        for gen_idx, line_tokens in enumerate(gen_lines):
            n_grams = ["_".join(gram) for gram in nltk.ngrams(line_tokens, n=gram_n)]
            all_ngram_num += len(n_grams)
            # for distinct
            for gram in n_grams:
                distinct_ngram[gram] += 1
            # for repetition
            for gram in set(n_grams):
                if n_grams.count(gram) >= repetition_times:
                    repetition_count += 1
                    break
        metrics[f"repetition-{gram_n}"] = "%.4f" % (repetition_count / float(len(gen_lines)))
        metrics[f"distinct-{gram_n}"] = "%.4f" % (len(distinct_ngram) / float(all_ngram_num))
    return metrics

class NGramCounter(object):
    def __init__(self, name="NGramCounter"):
        self.name = name
        self.ngram_wrapper = Counter()
        # self.download_resources()
        self.punkt_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')

    def download_resources(self):
        nltk.download("punkt")

    def get_ngram(self, input:str, gram_n:int) -> list:
        n_grams = ["_".join(gram) for gram in nltk.ngrams(input.strip().split(), n=gram_n)]
        return n_grams

    def get_distinct_ngram(self, input:str, gram_n:int) -> Counter:
        return Counter(self.get_ngram(input, gram_n))

    def sum_counter(self, c_list:List[Counter]):
        result = Counter()
        for one in c_list:
            result += one
        return result

    def calculate_intra_repetition(self, line:str, sent_limit:int, gram_n:int=3):
        intra_dists = dict()
        sents = self.punkt_tokenizer.tokenize(line)
        sents_n_grams = []

        for sent in sents[:sent_limit]:
            sents_n_grams.append(self.get_distinct_ngram(sent, gram_n))

        for idx in range(len(sents_n_grams)):
            if idx == 0:
                continue
            repeti = 0
            for prev_sents_n_grams in sents_n_grams[:idx]:
                intersect_size = len(prev_sents_n_grams & sents_n_grams[idx])
                repeti += float(intersect_size) * 100 / len(sents_n_grams[idx]) if len(sents_n_grams[idx]) > 0 else 0
            repeti /= idx
            intra_dists[idx+1] = repeti

        return intra_dists

    def parse_lines_for_intra_repetition(self, lines:List[str], sent_limit:int, gram_n:int=3):
        all_intra_metrics = {1: 0.0}
        for line in lines:
            intra_dists = self.calculate_intra_repetition(line=line, sent_limit=sent_limit, gram_n=gram_n)
            for key, val in intra_dists.items():
                if key not in all_intra_metrics:
                    all_intra_metrics[key] = []
                all_intra_metrics[key].append(val)
        for key_sent_idx in all_intra_metrics:
            all_intra_metrics[key_sent_idx] = np.mean(all_intra_metrics[key_sent_idx]).round(2)
        # all_intra_metrics[0] not counted
        intra_aggregate_score = round(sum(list(all_intra_metrics.values())) / (len(all_intra_metrics.keys()) - 1), 2)
        return all_intra_metrics, intra_aggregate_score

    def parse_lines_for_inter_repetition(self, lines:List[str], sent_limit:int, gram_n:int=3):
        all_inter_metrics = dict()
        dataset_sent_distinct_n_grams = dict()
        dataset_sent_n_grams_len = dict()
        for line in lines:
            sents = self.punkt_tokenizer.tokenize(line)[:sent_limit]
            for sent_idx in range(len(sents)):
                if sent_idx not in dataset_sent_distinct_n_grams:
                    dataset_sent_distinct_n_grams[sent_idx] = []
                    dataset_sent_n_grams_len[sent_idx] = []
                dataset_sent_distinct_n_grams[sent_idx].append(self.get_distinct_ngram(sents[sent_idx], gram_n))
                dataset_sent_n_grams_len[sent_idx].append(len(self.get_ngram(sents[sent_idx], gram_n)))

        for key_sent_idx in dataset_sent_distinct_n_grams:
            distinct_size = len(self.sum_counter(dataset_sent_distinct_n_grams[key_sent_idx]))
            all_grams_size = float(sum(dataset_sent_n_grams_len[key_sent_idx]))
            # repetition rate
            all_inter_metrics[key_sent_idx+1] = 1 - distinct_size / all_grams_size if all_grams_size > 0 else 0
            all_inter_metrics[key_sent_idx+1] = round(all_inter_metrics[key_sent_idx+1] * 100, 2)
        inter_aggregate_score = round(sum(list(all_inter_metrics.values())) / len(all_inter_metrics.keys()), 2)
        return all_inter_metrics, inter_aggregate_score